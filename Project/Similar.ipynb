{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity of descriptions (reviews)\n",
    "## Following W4\n",
    "* Shingles - k-shingle is any substring of length k found within the document.\n",
    "* Minhashing - Hashing each shingle to a number and keeping the minimum of the hash values.\n",
    "* Signatures - Minhashing for multiple hash functions and keeping the minhash values for each hash function in a list.\n",
    "* Jaccard similarity - The fraction of the number of elements in the intersection of two sets and the number of elements in the union of the two sets. Approximated by counting number of minhash values in the signature lists that match and dividing by the number of hash functions.\n",
    "* Locality-Sensitive Hashing (LSH) - Dividing the signature matrix into bands and hashing the bands. If two signatures are similar, they will hash to the same bucket with high probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import mmh3\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "      <th>Year</th>\n",
       "      <th>Length</th>\n",
       "      <th>Age</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>1994</td>\n",
       "      <td>2h 22m</td>\n",
       "      <td>15</td>\n",
       "      <td>9.3 (3M)</td>\n",
       "      <td>A banker convicted of uxoricide forms a friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>tt0068646</td>\n",
       "      <td>1972</td>\n",
       "      <td>2h 55m</td>\n",
       "      <td>15</td>\n",
       "      <td>9.2 (2.1M)</td>\n",
       "      <td>The aging patriarch of an organized crime dyna...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name         ID  Year  Length Age      Rating  \\\n",
       "0  The Shawshank Redemption  tt0111161  1994  2h 22m  15    9.3 (3M)   \n",
       "1             The Godfather  tt0068646  1972  2h 55m  15  9.2 (2.1M)   \n",
       "\n",
       "                                         Description  \n",
       "0  A banker convicted of uxoricide forms a friend...  \n",
       "1  The aging patriarch of an organized crime dyna...  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load descriptions from data/movies_metrics.csv\n",
    "Movies = pd.read_csv('data/movie_titles_and_ids.csv')\n",
    "Movies.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = Movies['Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (Nojan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import re \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_preprocess_text(corpus):\n",
    "    # Remove HTML tags\n",
    "    corpus = [BeautifulSoup(text, \"html.parser\").get_text() for text in corpus]\n",
    "\n",
    "    # Remove urls\n",
    "    corpus = [re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE) for text in corpus]\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    corpus = [re.sub(\"[^a-zA-Z]\", \" \", text) for text in corpus]\n",
    "\n",
    "    # Convert to lowercase\n",
    "    corpus = [text.lower() for text in corpus]\n",
    "\n",
    "    return corpus\n",
    "\n",
    "def preprocessing_lemmatization(corpus):\n",
    "    # Tokenize the text (split it into words)\n",
    "    corpus = [word_tokenize(text) for text in corpus]\n",
    "\n",
    "    # Remove stop words (The, a, on, etc)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    corpus = [[word for word in text if word not in stop_words] for text in corpus]\n",
    "\n",
    "    # Lemmatization,\n",
    "    # AKA remove word endings to get the base form\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    corpus = [[lemmatizer.lemmatize(word) for word in text] for text in corpus]\n",
    "\n",
    "    # Join the words back into one string\n",
    "    corpus = [\" \".join(text) for text in corpus]\n",
    "\n",
    "    return corpus\n",
    "\n",
    "descriptions = simple_preprocess_text(descriptions)\n",
    "descriptions = preprocessing_lemmatization(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle(s, q):\n",
    "    return {s[i:i+q] for i in range(len(s) - q + 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing and Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def listhash(shingle, seed):\n",
    "\treturn int(hashlib.sha256((shingle + str(seed)).encode('utf-8')).hexdigest(), 16) % 2**32-1\n",
    "\n",
    "def minhash(shingles, seed):\n",
    "    return min(listhash(s, seed) for s in shingles)\n",
    "\n",
    "def signature(shingles, k):\n",
    "\treturn [minhash(shingles, seed) for seed in range(k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signature for entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 5 # length of shingle  (5 ok for emails)\n",
    "k = 100 # number of minhashes\n",
    "keys = [f'{name} ({year})' for name, year in zip(Movies['Name'], Movies['Year'])]\n",
    "# docs = Movies.set_index('Name')['Description'].to_dict() # dictionary mapping document id to document contents\n",
    "docs = {key : desc for key, desc in zip(keys, descriptions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing that there will be an intersection of shingles in documents, this can be used to define the signatures for the entire dataset. Instead of iterating over documents and defining signatures individually, iterate over shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9105 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9105/9105 [00:05<00:00, 1544.59it/s]\n"
     ]
    }
   ],
   "source": [
    "def MakeSignatures(docs, shingles, U):\n",
    "    SIG = {doc : [float('inf') for _ in range(k)] for doc in docs}\n",
    "\n",
    "    for s in tqdm(U):\n",
    "        # hashes = signature(s, k)\n",
    "        hashes = [listhash(s, seed) for seed in range(k)]\n",
    "        for doc in docs:\n",
    "            if s in shingles[doc]:\n",
    "                for i in range(k):\n",
    "                    SIG[doc][i] = min(hashes[i], SIG[doc][i])\n",
    "    return SIG\n",
    "\n",
    "shingles = {doc: shingle(docs[doc], q) for doc in docs}\n",
    "U = set([s for doc in shingles for s in shingles[doc]])  # All shingles\n",
    "\n",
    "signatures = MakeSignatures(docs, shingles, U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(S, T):\n",
    "    intersection = len(S & T)\n",
    "    union = len(S | T)\n",
    "    return intersection / union\n",
    "\n",
    "# Approximate Jaccard similarity using minhash signatures\n",
    "def approximate_jaccard(A, B, signatures):\n",
    "    return sum(i == j for i, j in zip(signatures[A], signatures[B])) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shrek Forever After (2010) - Prince of Persia: The Sands of Time (2010)\n",
      "0.012145748987854251\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "A, B = random.sample(keys, 2)\n",
    "print(A, '-', B)\n",
    "\n",
    "doc1 = docs[A]\n",
    "doc2 = docs[B]\n",
    "shingle1 = shingle(doc1, q)\n",
    "shingle2 = shingle(doc2, q)\n",
    "signature1 = signature(shingle1, k)\n",
    "signature2 = signature(shingle2, k)\n",
    "\n",
    "# Exact Jaccard similarity\n",
    "print(jaccard(shingle1, shingle2))\n",
    "\n",
    "# Approximate Jaccard similarity\n",
    "print(approximate_jaccard(A, B, signatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15 ('Alice in Wonderland (2010)', 'Mystic River (2003)')\n"
     ]
    }
   ],
   "source": [
    "# Find most similar by description\n",
    "mx = 0.0\n",
    "most_similar = None\n",
    "for i, A in enumerate(keys):\n",
    "    for j in range(i+1, len(keys)):\n",
    "        B = keys[j]\n",
    "        # shingle1 = shingle(docs[A], q)\n",
    "        # shingle2 = shingle(docs[B], q)\n",
    "        # sim = jaccard(shingle1, shingle2)\n",
    "        sim = approximate_jaccard(A, B, signatures)\n",
    "        if sim > mx:\n",
    "            mx = sim\n",
    "            most_similar = (A, B)\n",
    "print(mx, most_similar)\n",
    "# Jaccard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Half-Blood Prince (2009) - Prince of Persia: The Sands of Time (2010)\n",
      "1.0\n",
      "100\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "A, B = random.sample(keys, 2)\n",
    "print(A, '-', B)\n",
    "# Exact Jaccard similarity\n",
    "print(jaccard(shingle1, shingle2))\n",
    "\n",
    "# Approximate Jaccard similarity\n",
    "matches = sum(1 for i, j in zip(signature1, signature2) if i == j)\n",
    "print(matches)\n",
    "print(matches / k)\n",
    "# jaccard(docs[A], docs[B]), approximate_jaccard(A, B, signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locality-Sensitive Hashing\n",
    "Pick $b$ and $r$ such that $k = br$ and $(1/b)^{1/r} \\approx s$, where $s$ is the similarity threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1414213562373095"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1/b)**(1/r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Diary of the Dead (2007)', 'Dead Snow (2009)', 0.11),\n",
       " ('The Sixth Sense (1999)', 'Good Will Hunting (1997)', 0.12),\n",
       " ('Resident Evil: Apocalypse (2004)',\n",
       "  'Resident Evil: Extinction (2007)',\n",
       "  0.14)]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fill_buckets(signatures, b, r):\n",
    "    buckets = {i: {} for i in range(b)}\n",
    "    for doc in signatures:\n",
    "        for i in range(b):\n",
    "            h = hash(tuple(signatures[doc][i*r:(i+1)*r]))\n",
    "            if h not in buckets[i]:\n",
    "                buckets[i][h] = []\n",
    "            buckets[i][h].append(doc)\n",
    "    return buckets\n",
    "\n",
    "def lsh(signatures, threshold):\n",
    "    b = 50\n",
    "    r = k // b\n",
    "    buckets = fill_buckets(signatures, b, r)\n",
    "\n",
    "    candidates = set()\n",
    "    for bucket in buckets:\n",
    "        for h in buckets[bucket]:\n",
    "            if len(buckets[bucket][h]) > 1:\n",
    "                for doc in buckets[bucket][h]:\n",
    "                    candidates.add(doc)\n",
    "    candidates = list(candidates)\n",
    "    \n",
    "    # Now compare all pairs of candidates\n",
    "    similar = []\n",
    "    for i, A in enumerate(candidates):\n",
    "        for j in range(i+1, len(candidates)):\n",
    "            B = candidates[j]\n",
    "            sim = approximate_jaccard(A, B, signatures)\n",
    "            if sim > threshold:\n",
    "                similar.append((A, B, sim))\n",
    "    return similar\n",
    "\n",
    "lsh(signatures, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
