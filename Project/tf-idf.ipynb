{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the movie recommendation idea, we want the end user to have an additional data point for their consideration of our recommendation. \n",
    "\n",
    "We have therefore decided to employ a sentiment analysis, based on movie reviews. The sentiment analysis will be based on a simple linear regression model, trained on a [sentiment-classified IMDB movie review dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). As part of this, the movie reviews will be preprocessed, and vectorized using TF-IDF. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial imports and data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nojan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nojan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Nojan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df=pd.read_csv(\"training_data.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "# Reviewing base characteristics of the data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 50K reviews, labeled positive/negative. Luckily, the dataset seems well-made as there are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "The next step is to do some preprocessing, to reduce number of features/dimensionality (I.E cleaning up the text and boiling it down to the bare minimum needed for the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "reviews =[review for review in df['review']]\n",
    "#preprocessing\n",
    "\n",
    "def simple_preprocess_text(corpus):\n",
    "    # Remove HTML tags\n",
    "    corpus = [BeautifulSoup(text, \"html.parser\").get_text() for text in corpus]\n",
    "\n",
    "    # Remove urls\n",
    "    corpus = [re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE) for text in corpus]\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    corpus = [re.sub(\"[^a-zA-Z]\", \" \", text) for text in corpus]\n",
    "\n",
    "    # Convert to lowercase\n",
    "    corpus = [text.lower() for text in corpus]\n",
    "\n",
    "    return corpus\n",
    "\n",
    "def preprocessing_lemmatization(corpus):\n",
    "    # Tokenize the text (split it into words)\n",
    "    corpus = [word_tokenize(text) for text in corpus]\n",
    "\n",
    "    # Remove stop words (The, a, on, etc)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    corpus = [[word for word in text if word not in stop_words] for text in corpus]\n",
    "\n",
    "    # Lemmatization,\n",
    "    # AKA remove word endings to get the base form\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    corpus = [[lemmatizer.lemmatize(word) for word in text] for text in corpus]\n",
    "\n",
    "    # Join the words back into one string\n",
    "    corpus = [\" \".join(text) for text in corpus]\n",
    "\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<p>It's a great movie. I love it! <a href='http://www.google.com'>Google</a></p>\"]\n",
      "['it s a great movie  i love it  google']\n",
      "['great movie love google']\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE of pre-processing some text\n",
    "sample_text = [\"<p>It's a great movie. I love it! <a href='http://www.google.com'>Google</a></p>\"]\n",
    "print(sample_text)\n",
    "\n",
    "test= simple_preprocess_text(sample_text)\n",
    "print(test)\n",
    "test= preprocessing_lemmatization(test)\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nojan\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processed_text= simple_preprocess_text(reviews)\n",
    "processed_text= preprocessing_lemmatization(processed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the text is now cleaned up, and only the most important words for the sentiment analysis is kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to use TF-IDF to extract the most important features from the text. TF stands for term frequency, the frequency of a word appearing in a document. IDF stands for Inverse Document Frequency, which is a weighing method that indicate how commonly a word appears across all the documents, which in our case are the reviews. \n",
    "\n",
    "#### TODO: maybe include math of the 2 weighting techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization,\n",
    "# which is a way to represent text data as a matrix of numbers\n",
    "vector = TfidfVectorizer(max_features=5000)\n",
    "X = vector.fit_transform(processed_text).toarray()\n",
    "# We will also include the sentiment column as the target variable\n",
    "y = df['sentiment'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "# 80% of the data will be used for training and 20% for testing\n",
    "# The random_state parameter is used to ensure that the data is split in the same way every time the code is run\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be training and testing on different model types, to evaluate which one is best for our purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a logistic regression model\n",
    "models['Logistic Regression'] = LogisticRegression()\n",
    "models['Logistic Regression'].fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a SVM model\n",
    "from sklearn.svm import SVC\n",
    "models['SVM'] = SVC(kernel='linear') # faster converge, if linear\n",
    "models['SVM'].fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "models['Naive Bayes'] = GaussianNB()\n",
    "models['Naive Bayes'].fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sentiment predictions: 0.8878\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model in models:\n",
    "    y_pred = models[model].predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Accuracy of sentiment prediction: {accuracy}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
